{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccc9e22a",
        "outputId": "bf319e50-a0d1-4025-ebab-b4b09189f039"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: pypdf in ./.venv/lib/python3.13/site-packages (6.0.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install -q langchain langchain-community transformers sentence-transformers faiss-cpu requests\n",
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76665290",
        "outputId": "bfb48549-1e11-46a4-f2f7-48f3e2ed3719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“š Loading documents...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33f613d8a163415ba04900f2b5824137",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/gp/crhzv8h912sgp6zjmjhn917m0000gp/T/ipykernel_10993/3606727322.py:154: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”Ž Creating embeddings...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a44fed97b294027a19b02a152d67845",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¤– Loading model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ‘‰ Running in Notebook mode. Use chat('your question').\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import requests\n",
        "import tempfile\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from transformers import pipeline\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import sys\n",
        "\n",
        "# ---------------------------\n",
        "# Sources\n",
        "# ---------------------------\n",
        "text_sources = {\n",
        "    'I.P.C': 'https://github.com/SaiSudheerKankanala/SAIbot/raw/main/ipc.pdf',\n",
        "    'Constitution': 'https://github.com/SaiSudheerKankanala/SAIbot/raw/main/indian%20constitution.pdf',\n",
        "    'Garuda': 'https://github.com/SaiSudheerKankanala/SAIbot/raw/main/GarudaPurana.pdf',\n",
        "    'Bhagavad Gita': 'https://github.com/SaiSudheerKankanala/SAIbot/raw/main/Bhagavad-gita_As_It_Is.pdf',\n",
        "    'Quran': 'https://github.com/SaiSudheerKankanala/SAIbot/raw/main/quran-allah.pdf'\n",
        "}\n",
        "\n",
        "# ---------------------------\n",
        "# Download + Load PDFs\n",
        "# ---------------------------\n",
        "def load_pdf(url, source_name):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n",
        "        tmp.write(response.content)\n",
        "        tmp_path = tmp.name\n",
        "    loader = PyPDFLoader(tmp_path)\n",
        "    docs = loader.load()\n",
        "    for d in docs:\n",
        "        d.metadata[\"source\"] = source_name\n",
        "    return docs\n",
        "\n",
        "print(\"ðŸ“š Loading documents...\")\n",
        "all_docs = {}\n",
        "for name, url in tqdm(text_sources.items()):\n",
        "    all_docs[name] = load_pdf(url, name)\n",
        "\n",
        "# ---------------------------\n",
        "# Split documents into chunks\n",
        "# ---------------------------\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=400,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# Build FAISS vector DBs\n",
        "# ---------------------------\n",
        "vector_dbs = {}\n",
        "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "print(\"ðŸ”Ž Creating embeddings...\")\n",
        "for name, docs in tqdm(all_docs.items()):\n",
        "    chunks = text_splitter.split_documents(docs)\n",
        "    if chunks:\n",
        "        vector_dbs[name] = FAISS.from_documents(chunks, embedding=embedder)\n",
        "\n",
        "# ---------------------------\n",
        "# Local LLM (FLAN-T5)\n",
        "# ---------------------------\n",
        "print(\"ðŸ¤– Loading model...\")\n",
        "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
        "\n",
        "def generate_answer(question, context):\n",
        "    prompt = f\"\"\"\n",
        "    You are a helpful assistant.\n",
        "    Answer the question ONLY if it is clearly answered in the given context.\n",
        "    If the context is unrelated or unclear, respond with exactly: \"Not mentioned in this source.\"\n",
        "\n",
        "    Question: {question}\n",
        "    Context: {context}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "    return generator(prompt, max_new_tokens=200, clean_up_tokenization_spaces=True)[0]['generated_text']\n",
        "\n",
        "# ---------------------------\n",
        "# QA Function\n",
        "# ---------------------------\n",
        "def answer_question(question, k=2):\n",
        "    final_output = []\n",
        "    for source_name in text_sources.keys():\n",
        "        if source_name in vector_dbs:\n",
        "            docs = vector_dbs[source_name].similarity_search(question, k=k)\n",
        "            if docs:\n",
        "                context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "                answer = generate_answer(question, context).strip()\n",
        "                if answer and answer != \"Not mentioned in this source.\":\n",
        "                    final_output.append(f\"According to {source_name.capitalize()}: {answer}\")\n",
        "    return \"\\n\\n\".join(final_output) if final_output else \"No relevant answer found in any source.\"\n",
        "\n",
        "# ---------------------------\n",
        "# Chat Helpers\n",
        "# ---------------------------\n",
        "def chat(question: str):\n",
        "    \"\"\"Ask a question and get an answer (for notebooks).\"\"\"\n",
        "    return answer_question(question)\n",
        "\n",
        "def interactive_chat():\n",
        "    \"\"\"Interactive loop (for terminal).\"\"\"\n",
        "    print(\"Bot: Hello! Ask me anything. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        question = input(\"You: \")\n",
        "        if question.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "            print(\"Bot: Goodbye!\")\n",
        "            break\n",
        "        print(\"Bot:\", answer_question(question))\n",
        "\n",
        "# Detect environment (Notebook vs Script)\n",
        "def is_notebook():\n",
        "    try:\n",
        "        shell = get_ipython().__class__.__name__\n",
        "        if shell == \"ZMQInteractiveShell\":\n",
        "            return True   # Jupyter notebook\n",
        "        elif shell == \"TerminalInteractiveShell\":\n",
        "            return False  # IPython terminal\n",
        "        else:\n",
        "            return False\n",
        "    except NameError:\n",
        "        return False      # Standard Python script\n",
        "\n",
        "# ---------------------------\n",
        "# Entry Point\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    if is_notebook():\n",
        "        print(\"ðŸ‘‰ Running in Notebook mode. Use chat('your question').\")\n",
        "    else:\n",
        "        interactive_chat()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"According to Bhagavad gita: Discharge of One's Prescribed Duty in Krsna Consciousness\""
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat(\"What is Dharma in Bhagavad Gita?\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
